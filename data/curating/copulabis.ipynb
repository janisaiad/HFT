{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import polars as pl\n",
    "import dotenv\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "FOLDER_PATH = os.getenv(\"FOLDER_PATH\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "# +\n",
    "# Get list of all stocks from .env\n",
    "stocks_list = [\"GOOGL\", \"AAPL\", \"AMZN\", \"AAL\", \"MSFT\", \"GT\", \"INTC\", \"IOVA\", \"PTEN\", \n",
    "               \"MLCO\", \"PTON\", \"VLY\", \"VOD\", \"CSX\", \"WB\", \"BGC\", \"GRAB\", \"KHC\", \"HLMN\",\n",
    "               \"IEP\", \"GBDC\", \"WBD\", \"PSNY\", \"NTAP\", \"GEO\", \"LCID\", \"GCMG\", \"CXW\", \n",
    "               \"RIOT\", \"HL\", \"CX\", \"ERIC\", \"UA\"]\n",
    "\n",
    "# Get parquet files for each stock and count occurrences of each date\n",
    "stock_files = {}\n",
    "date_counts = {}\n",
    "date_stocks = {}  # Dictionary to store stocks for each date\n",
    "for stock in stocks_list:\n",
    "    files = [f for f in os.listdir(f\"{FOLDER_PATH}{stock}\") if f.endswith('.parquet')]\n",
    "    files.sort()\n",
    "    stock_files[stock] = set(files)\n",
    "    \n",
    "    # Count occurrences of each date and store stocks\n",
    "    for file in files:\n",
    "        # Extract date from filename (remove stock prefix and .parquet suffix)\n",
    "        date = file.replace(f\"{stock}_\", \"\").replace(\".parquet\", \"\")\n",
    "        if date in date_counts:\n",
    "            date_counts[date] += 1\n",
    "            date_stocks[date].append(stock)\n",
    "        else:\n",
    "            date_counts[date] = 1\n",
    "            date_stocks[date] = [stock]\n",
    "\n",
    "# Find the most common date\n",
    "print(date_counts)\n",
    "print(\"Stocks for each date:\", date_stocks)\n",
    "most_common_date = max(date_counts.items(), key=lambda x: x[1])[0]\n",
    "print(f\"Most common date across stocks: {most_common_date}\")\n",
    "\n",
    "# -\n",
    "\n",
    "#\n",
    "\n",
    "# +\n",
    "# Get all dates with maximum count\n",
    "max_count = max(date_counts.values())\n",
    "most_common_dates = [date for date, count in date_counts.items() if count == max_count]\n",
    "print(f\"Dates with maximum count ({max_count} stocks): {most_common_dates}\")\n",
    "print(\"stocks for most common date:\", date_stocks[most_common_dates[0]])\n",
    "# Create empty list to store all dataframes\n",
    "all_dfs = {stock: pl.DataFrame() for stock in date_stocks[most_common_dates[0]]}\n",
    "\n",
    "# Load and combine data for each date\n",
    "for date in most_common_dates[:1]:\n",
    "    print(f\"\\nProcessing date: {date}\")\n",
    "    stocks_for_date = date_stocks[date]\n",
    "    \n",
    "    # Load data for each stock on this date\n",
    "    for stock in tqdm(stocks_for_date):\n",
    "        file_path = f\"{FOLDER_PATH}{stock}/{stock}_{date}.parquet\"\n",
    "        if os.path.exists(file_path):\n",
    "            df = pl.read_parquet(file_path)\n",
    "            all_dfs[stock] = pl.concat([all_dfs[stock], df])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def curate_mid_price(df,stock):\n",
    "    if \"publisher_id\" in df.columns:\n",
    "        num_entries_by_publisher = df.group_by(\"publisher_id\").len().sort(\"len\", descending=True)\n",
    "        if len(num_entries_by_publisher) > 1:\n",
    "                df = df.filter(pl.col(\"publisher_id\") == 41)\n",
    "        \n",
    "        \n",
    "    if stock == \"GOOGL\":\n",
    "        df = df.filter(pl.col(\"ts_event\").dt.hour() >= 13)\n",
    "        df = df.filter(pl.col(\"ts_event\").dt.hour() <= 20)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        df = df.filter(\n",
    "            (\n",
    "                (pl.col(\"ts_event\").dt.hour() == 9) & (pl.col(\"ts_event\").dt.minute() >= 35) |\n",
    "                (pl.col(\"ts_event\").dt.hour() > 9) & (pl.col(\"ts_event\").dt.hour() < 16)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Remove the first row at 9:30\n",
    "    df = df.with_row_index(\"index\").filter(\n",
    "        ~((pl.col(\"ts_event\").dt.hour() == 9) & \n",
    "          (pl.col(\"ts_event\").dt.minute() == 30) & \n",
    "          (pl.col(\"index\") == df.filter(\n",
    "              (pl.col(\"ts_event\").dt.hour() == 9) & \n",
    "              (pl.col(\"ts_event\").dt.minute() == 30)\n",
    "          ).with_row_index(\"index\").select(\"index\").min())\n",
    "        )\n",
    "    ).drop(\"index\")\n",
    "    mid_price = (df[\"ask_px_00\"] + df[\"bid_px_00\"]) / 2\n",
    "    \n",
    "    # managing nans or infs, preceding value filling\n",
    "    mid_price = mid_price.fill_nan(mid_price.shift(1))\n",
    "    df = df.with_columns(mid_price=mid_price)\n",
    "    # sort by ts_event\n",
    "    # added microprice\n",
    "    microprice = (df[\"ask_px_00\"]*df[\"bid_sz_00\"] + df[\"bid_px_00\"]*df[\"ask_sz_00\"]) / (df[\"ask_sz_00\"] + df[\"bid_sz_00\"])\n",
    "    # remove nans or infs\n",
    "    microprice = microprice.fill_nan(microprice.shift(1))\n",
    "    df = df.with_columns(microprice=microprice)\n",
    "    df = df.sort(\"ts_event\")\n",
    "    return df\n",
    "\n",
    "\n",
    "for stock in tqdm(date_stocks[most_common_dates[0]], \"Huge amount of data to process\"):\n",
    "        df = all_dfs[stock]\n",
    "        df  = curate_mid_price(df,stock)\n",
    "        all_dfs[stock] = df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each day, we fit a copula for mid price variations depending on their stock\n",
    "for date in most_common_dates:\n",
    "    print(f\"\\nProcessing date: {date}\")\n",
    "    \n",
    "    # Sample at different time scales\n",
    "    time_scales = [\"30us\", \"100us\", \"1ms\", \"10ms\", \"100ms\", \"1s\"]\n",
    "    time_scales.reverse()\n",
    "    \n",
    "    for time_scale in time_scales:\n",
    "        print(f\"\\nProcessing time scale: {time_scale}\")\n",
    "        \n",
    "        # Sample each stock at regular intervals and calculate returns\n",
    "        stock_returns = {}\n",
    "        for stock in date_stocks[date]:\n",
    "            print(f\"Processing stock: {stock}\")\n",
    "            df = all_dfs[stock]\n",
    "            \n",
    "            # Resample data at regular intervals using group_by_dynamic\n",
    "            sampled_prices = df.group_by_dynamic(\n",
    "                \"ts_event\",\n",
    "                every=time_scale\n",
    "            ).agg([\n",
    "                pl.col(\"microprice\").last().alias(\"microprice\")\n",
    "            ])\n",
    "            \n",
    "            # Calculate log returns\n",
    "            returns = np.diff(np.log(sampled_prices[\"microprice\"].to_numpy()))\n",
    "            stock_returns[stock] = returns\n",
    "            print(f\"Number of data points for {stock}: {len(returns)}\")\n",
    "        \n",
    "        # Ensure all stocks have same number of returns\n",
    "        min_length = min(len(returns) for returns in stock_returns.values())\n",
    "        for stock in stock_returns:\n",
    "            stock_returns[stock] = stock_returns[stock][:min_length]\n",
    "        \n",
    "        print(f\"\\nAll stocks trimmed to {min_length} data points\")\n",
    "        \n",
    "        # Fit different types of copulas\n",
    "        from copulalib.copulalib import Copula\n",
    "        \n",
    "        copula_types = {\n",
    "            'Clayton': 'clayton',\n",
    "            'Frank': 'frank', \n",
    "            'Gumbel': 'gumbel'\n",
    "        }\n",
    "        \n",
    "        print(\"\\nFitting copulas...\")\n",
    "        stocks = list(stock_returns.keys())\n",
    "        for i in range(len(stocks)):\n",
    "            for j in range(i+1, len(stocks)):\n",
    "                stock1, stock2 = stocks[i], stocks[j]\n",
    "                print(f\"\\nAnalyzing pair: {stock1} - {stock2}\")\n",
    "                \n",
    "                for copula_name, family in copula_types.items():\n",
    "                    try:\n",
    "                        copula = Copula(stock_returns[stock1], stock_returns[stock2], family=family)\n",
    "                        \n",
    "                        kendall_tau = copula.tau\n",
    "                        spearman_corr = copula.sr\n",
    "                        pearson_corr = copula.pr\n",
    "                        theta = copula.theta\n",
    "                        \n",
    "                        X1, Y1 = copula.generate_xy(1000)\n",
    "                        \n",
    "                        print(f\"\\nResults for {copula_name} copula:\")\n",
    "                        print(f\"Theta: {theta:.3f}\")\n",
    "                        print(f\"Kendall tau: {kendall_tau:.3f}\")\n",
    "                        print(f\"Spearman correlation: {spearman_corr:.3f}\")\n",
    "                        print(f\"Pearson correlation: {pearson_corr:.3f}\")\n",
    "                        \n",
    "                        plt.figure(figsize=(12, 8))\n",
    "                        plt.scatter(X1, Y1, alpha=0.5)\n",
    "                        plt.xlabel('U')\n",
    "                        plt.ylabel('V')\n",
    "                        \n",
    "                        info_text = [\n",
    "                            f'Theta: {theta:.3f}',\n",
    "                            f'Kendall tau: {kendall_tau:.3f}',\n",
    "                            f'Spearman corr: {spearman_corr:.3f}',\n",
    "                            f'Pearson corr: {pearson_corr:.3f}'\n",
    "                        ]\n",
    "                        \n",
    "                        plt.text(0.05, 0.95,\n",
    "                                '\\n'.join(info_text),\n",
    "                                transform=plt.gca().transAxes,\n",
    "                                bbox=dict(facecolor='white', alpha=0.8))\n",
    "                        \n",
    "                        plt.title(f\"{copula_name} Copula - {stock1} vs {stock2}\\n{date} - Scale {time_scale}\")\n",
    "                        plt.show()\n",
    "                        plt.close()\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error fitting {copula_name} copula: {e}\")\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
